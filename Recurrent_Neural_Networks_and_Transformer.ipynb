{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2a6e9e8b",
      "metadata": {
        "id": "2a6e9e8b"
      },
      "source": [
        "## Recurrent Neural Networks and Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f7493e6",
      "metadata": {
        "id": "7f7493e6"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "193d314b",
      "metadata": {
        "id": "193d314b"
      },
      "source": [
        "<font size='4'>In this project you will practice putting together implementations of Recurrent Neural Networks, Transformer (encoder and decoder), and their applications to text classification, image classification, and text generation (machine translation). Especially for Transformer, you will get good understandings about foundations for very state-of-the-art models that you likely to see in tech news articles nowadays, like GPT-3, CLIP, or VisionTransformer.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c34a5320",
      "metadata": {
        "id": "c34a5320"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e59bfb8",
      "metadata": {
        "id": "3e59bfb8"
      },
      "outputs": [],
      "source": [
        "# ==================\n",
        "# ATTENTION:\n",
        "# For the first time, uncomment the following lines and re-run the cell.\n",
        "# You may have to hit the \"RESTART RUNTIME\" button in the output and then re-run this cell.\n",
        "# Once it is done, you can comment the following lines and run other cells.\n",
        "# ==================\n",
        "\n",
        "# # Create source and target language tokenizer. Make sure to install the dependencies.\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d789993e",
      "metadata": {
        "id": "d789993e"
      },
      "source": [
        "## Part 1: Text Classification with RNN (24 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92081dc6",
      "metadata": {
        "id": "92081dc6"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.datasets import AG_NEWS\n",
        "train_iter = AG_NEWS(split='train')\n",
        "\n",
        "# Let's check what the data looks like\n",
        "print(len(train_iter))\n",
        "print(next(train_iter))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28eb6bd7",
      "metadata": {
        "id": "28eb6bd7"
      },
      "source": [
        "### <font size='4' color='red'>Task 1.1: Implement a RNNCell (4 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2db54f12",
      "metadata": {
        "id": "2db54f12"
      },
      "outputs": [],
      "source": [
        "# Documentation of nn.Module https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module\n",
        "class RNNCell(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    RNNCell is a single cell that takes x_t and h_{t_1} as input and outputs h_t.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dim: int):\n",
        "        \"\"\"\n",
        "        Constructor of RNNCell.\n",
        "\n",
        "        Inputs:\n",
        "        - input_dim: Dimension of the input x_t\n",
        "        - hidden_dim: Dimension of the hidden state h_{t-1} and h_t\n",
        "        \"\"\"\n",
        "\n",
        "        # We always need to do this step to properly implement the constructor why????\n",
        "        super(RNNCell, self).__init__()\n",
        "\n",
        "        self.linear_x, self.linear_h, self.non_linear = None, None, None\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO: Define the linear transformation layers for x_t and h_{t-1} and   #\n",
        "        # the non-linear layer. You can use tanh here.                            #\n",
        "        ###########################################################################\n",
        "\n",
        "        self.linear_x= nn.Linear(input_dim,hidden_dim)\n",
        "        self.linear_h = nn.Linear(hidden_dim,hidden_dim)\n",
        "        self.non_linear = nn.Tanh()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "    def forward(self, x_cur: torch.Tensor, h_prev: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Compute h_t given x_t and h_{t-1}.\n",
        "\n",
        "        Inputs:\n",
        "        - x_cur: x_t, a tensor with the same of BxC, where B is the batch size and\n",
        "          C is the channel dimension.\n",
        "        - h_prev: h_{t-1}, a tensor with the same of BxH, where H is the channel\n",
        "          dimension.\n",
        "        \"\"\"\n",
        "        h_cur= None\n",
        "        ###########################################################################\n",
        "        # TODO: Define the linear transformation layers for x_t and h_{t-1} and   #\n",
        "        # the non-linear layer.                                                   #\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "        linear_x=self.linear_x(x_cur)\n",
        "        linear_h=self.linear_h(h_prev)\n",
        "        h_cur= self.non_linear(linear_x+linear_h)\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "        return h_cur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b62f7e2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b62f7e2d",
        "outputId": "a8f4a4c5-4e5b-4b88-ffb7-6098350f9df1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 16])\n"
          ]
        }
      ],
      "source": [
        "# Let's run a sanity check of your model\n",
        "x = torch.randn((2, 8))\n",
        "h = torch.randn((2, 16))\n",
        "model = RNNCell(8, 16)\n",
        "y = model(x, h)\n",
        "assert len(y.shape) == 2 and y.shape[0] == 2 and y.shape[1] == 16\n",
        "print(y.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb2ee43c",
      "metadata": {
        "id": "cb2ee43c"
      },
      "source": [
        "### <font size='4' color='red'>Task 1.2: Implement a single-layer (single-stack) RNN (5 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0120521",
      "metadata": {
        "id": "f0120521"
      },
      "outputs": [],
      "source": [
        "class RNN(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    RNN is a single-layer (stack) RNN by connecting multiple RNNCell together in a single\n",
        "    direction, where the input sequence is processed from left to right.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dim: int):\n",
        "        \"\"\"\n",
        "        Constructor of the RNN module.\n",
        "\n",
        "        Inputs:\n",
        "        - input_dim: Dimension of the input x_t\n",
        "        - hidden_dim: Dimension of the hidden state h_{t-1} and h_t\n",
        "        \"\"\"\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO: Define the RNNCell.                                               #\n",
        "        ###########################################################################\n",
        "\n",
        "        self.RNNCell = RNNCell(input_dim, hidden_dim)\n",
        "\n",
        "\n",
        "        # raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Compute the hidden representations for every token in the input sequence.\n",
        "\n",
        "        Input:\n",
        "        - x: A tensor with the shape of BxLxC, where B is the batch size, L is the squence\n",
        "          length, and C is the channel dimmension\n",
        "\n",
        "        Return:\n",
        "        - h: A tensor with the shape of BxLxH, where H is the hidden dimension of RNNCell\n",
        "        \"\"\"\n",
        "        b = x.shape[0]\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        # initialize the hidden dimension\n",
        "        init_h = x.new_zeros((b, self.hidden_dim))\n",
        "\n",
        "        h = x.new_zeros((b, seq_len, self.hidden_dim))\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO: Compute the hidden representation for every token in the input    #\n",
        "        # from left to right.\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "        for i in range(seq_len):\n",
        "\n",
        "          if i==0:\n",
        "            h [:,i,:]= self.RNNCell(x[:,i,:],h [:,i,:])\n",
        "          else:\n",
        "            h [:,i,:]= self.RNNCell(x[:,i,:],h [:,i-1,:])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "        return h\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfb661d8",
      "metadata": {
        "id": "dfb661d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ff17c2a-9200-40cf-e908-9fcca228cf35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10, 16])\n"
          ]
        }
      ],
      "source": [
        "# Let's run a sanity check of your model\n",
        "x = torch.randn((2, 10, 8))\n",
        "model = RNN(8, 16)\n",
        "y = model(x)\n",
        "assert len(y.shape) == 3\n",
        "for dim, dim_gt in zip(y.shape, [2, 10, 16]):\n",
        "    assert dim == dim_gt\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90f8ec47",
      "metadata": {
        "id": "90f8ec47"
      },
      "source": [
        "### <font size='4' color='red'>Task 1.3: Implement a RNN-based text classifier (4 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed42889c",
      "metadata": {
        "id": "ed42889c"
      },
      "outputs": [],
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    A RNN-based classifier for text classification. It first converts tokens into word embeddings.\n",
        "    And then feeds the embeddings into a RNN, where the hidden representations of all tokens are\n",
        "    then averaged to get a single embedding of the sentence. It will be used as input to a linear\n",
        "    classifier.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "            vocab_size: int, embed_dim: int, rnn_hidden_dim: int, num_class: int, pad_token: int\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Constructor.\n",
        "\n",
        "        Inputs:\n",
        "        - vocab_size: Vocabulary size, indicating how many tokens we have in total.\n",
        "        - embed_dim: The dimension of word embeddings\n",
        "        - rnn_hidden_dim: The hidden dimension of the RNN.\n",
        "        - num_class: Number of classes.\n",
        "        - pad_token: The index of the padding token.\n",
        "        \"\"\"\n",
        "        super(RNNClassifier, self).__init__()\n",
        "\n",
        "        # word embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_token)\n",
        "        self.vocab_size=vocab_size\n",
        "        self.rnn, self.fc = None, None\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO: Define the RNN and the classification layer.                      #\n",
        "        ###########################################################################\n",
        "\n",
        "        self.rnn = RNN(embed_dim,rnn_hidden_dim)\n",
        "\n",
        "        self.avg=torch.nn.AdaptiveAvgPool2d((1,rnn_hidden_dim))\n",
        "\n",
        "        self.fc = nn.Linear(rnn_hidden_dim,num_class)\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text):\n",
        "        \"\"\"\n",
        "        Get classification scores (logits) of the input.\n",
        "\n",
        "        Input:\n",
        "        - text: Tensor with the shape of BxLxC.\n",
        "\n",
        "        Return:\n",
        "        - logits: Tensor with the shape of BxK, where K is the number of classes\n",
        "        \"\"\"\n",
        "\n",
        "        # get word embeddings\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        logits = None\n",
        "        ###########################################################################\n",
        "        # TODO: Compute logits of the input.                                      #\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "\n",
        "        y=self.rnn(embedded)\n",
        "\n",
        "        yy=self.avg(y)\n",
        "        yy=yy.reshape(yy.shape[0],yy.shape[-1])\n",
        "\n",
        "        logits= self.fc(yy)\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce2d2044",
      "metadata": {
        "id": "ce2d2044"
      },
      "outputs": [],
      "source": [
        "# Let's run a sanity check of your model\n",
        "vocab_size = 10\n",
        "embed_dim = 16\n",
        "rnn_hidden_dim = 32\n",
        "num_class = 3\n",
        "\n",
        "x = torch.arange(vocab_size).view(1, -1)\n",
        "x = torch.cat((x, x), dim=0)\n",
        "print('x.shape: {}'.format(x.shape))\n",
        "model = RNNClassifier(vocab_size, embed_dim, rnn_hidden_dim, num_class, 0)\n",
        "y = model(x)\n",
        "assert len(y.shape) == 2 and y.shape[0] == 2 and y.shape[1] == num_class\n",
        "print(y.shape)\n",
        "model = model.to('cuda:0')\n",
        "x = x.to('cuda:0')\n",
        "y = model(x)\n",
        "print(y.shape, y.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "612890ad",
      "metadata": {
        "id": "612890ad"
      },
      "source": [
        "### Set up data related stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c92ea24e",
      "metadata": {
        "id": "c92ea24e"
      },
      "outputs": [],
      "source": [
        "# check here for details https://github.com/pytorch/text/blob/main/torchtext/data/utils.py#L52-#L166\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "# check here for details https://github.com/pytorch/text/blob/main/torchtext/vocab/vocab_factory.py#L65-L113\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# A tokenizer splits a input setence into a set of tokens, including those puncuation\n",
        "# For example\n",
        "# >>> tokens = tokenizer(\"You can now install TorchText using pip!\")\n",
        "# >>> tokens\n",
        "# >>> ['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "train_iter = AG_NEWS(split='train')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "# Creates a vocab object which maps tokens to indices\n",
        "# Check here for details https://github.com/pytorch/text/blob/main/torchtext/vocab/vocab.py\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "\n",
        "# The specified token will be returned when a out-of-vocabulary token is queried.\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1\n",
        "\n",
        "# The padding token we need to use\n",
        "# The returned indices are always in an array\n",
        "PAD_TOKEN = vocab(tokenizer('<pad>'))\n",
        "assert len(PAD_TOKEN) == 1\n",
        "PAD_TOKEN = PAD_TOKEN[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f13c252",
      "metadata": {
        "id": "0f13c252"
      },
      "source": [
        "### <font size='4' color='red'>Task 1.4: Collate Batched Data with Data Loaders (4 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdd006c5",
      "metadata": {
        "id": "bdd006c5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Documentation of DataLoader https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Merges a list of samples to form a mini-batch of Tensor(s)\n",
        "def collate_batch(batch):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "    - batch: A list of data in a mini batch, where the length denotes the batch size.\n",
        "      The actual context depends on a particular dataset. In our case, each position\n",
        "      contains a label and a Tensor (tokens in a sentence).\n",
        "\n",
        "    Returns:\n",
        "    - batched_label: A Tensor with the shape of (B,)\n",
        "    - batched_text: A Tensor with the shape of (B, L, C), where L is the sequence length\n",
        "      and C is the channeld dimension\n",
        "    \"\"\"\n",
        "    label_list, text_list, text_len_list = [], [], []\n",
        "\n",
        "    for (_label, _text) in batch:\n",
        "\n",
        "        label_list.append(label_pipeline(_label))\n",
        "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        text_len_list.append(processed_text.size(0))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    batched_label, batched_text = None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Pad the text tensor in the mini batch so that they have the same  #\n",
        "    # length. Specifically, you need to calculate the maximum length in the   #\n",
        "    # batch and then add the token PAD_TOKEN to the end of those              #\n",
        "    # shorter sentences.                                                      #\n",
        "    ###########################################################################\n",
        "\n",
        "    max_length = max (text_len_list)\n",
        "    num=len(label_list)\n",
        "\n",
        "    batched_text = torch.zeros((num,max_length),dtype=torch.int64)\n",
        "    batched_label = torch.zeros((num,1),dtype=torch.int64)\n",
        "\n",
        "\n",
        "    for i in range (len(label_list)):\n",
        "\n",
        "      pad_num =  max_length - text_len_list[i]\n",
        "      tst=text_list[i]\n",
        "      padd = torch.zeros(pad_num,dtype=torch.int64)\n",
        "      rr=torch.cat((tst, padd), dim=0)\n",
        "      batched_text[i] =rr\n",
        "      batched_label[i] = label_list[i]\n",
        "\n",
        "\n",
        "\n",
        "    #raise NotImplementedError\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "\n",
        "    return batched_label.long(), batched_text.long()\n",
        "\n",
        "# Now, let's check what the batched data looks like\n",
        "train_iter = AG_NEWS(split='train')\n",
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
        "for idx, (label, data) in enumerate(dataloader):\n",
        "    if idx > 0:\n",
        "        break\n",
        "    print('label.shape: {}'.format(label.shape))\n",
        "    print('label: {}'.format(label))\n",
        "    print('data.shape: {}'.format(data.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5c3571e",
      "metadata": {
        "id": "a5c3571e"
      },
      "source": [
        "### <font size='4' color='red'>Task 1.5: Functions of training for a single epoch and evaluation (4 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5dc8d4a",
      "metadata": {
        "id": "b5dc8d4a"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def train(model, dataloader, loss_func, device, grad_norm_clip):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text) in enumerate(dataloader):\n",
        "        label = label.to(device)\n",
        "        text = text.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = None\n",
        "        ###########################################################################\n",
        "        # TODO: compute the logits of the input, get the loss, and do the         #\n",
        "        # gradient backpropagation.\n",
        "        ###########################################################################\n",
        "\n",
        "        logits = model(text)\n",
        "\n",
        "        label = torch.reshape(label,(label.shape[0],))\n",
        "\n",
        "        loss = loss_func(logits, label)\n",
        "        loss.backward()  # compute new gradients\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "        torch.autograd.set_detect_anomaly(True)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm_clip)\n",
        "        optimizer.step()\n",
        "        total_acc += (logits.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(model, dataloader, loss_func, device):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text) in enumerate(dataloader):\n",
        "            label = label.to(device)\n",
        "            text = text.to(device)\n",
        "\n",
        "\n",
        "            ###########################################################################\n",
        "            # TODO: compute the logits of the input, get the loss.                    #\n",
        "            ###########################################################################\n",
        "            logits = model(text)\n",
        "\n",
        "\n",
        "            label = torch.reshape(label,(label.shape[0],))\n",
        "=\n",
        "            loss = loss_func(logits, label)\n",
        "\n",
        "\n",
        "            #raise NotImplementedError\n",
        "            ###########################################################################\n",
        "            #                             END OF YOUR CODE                            #\n",
        "            ###########################################################################\n",
        "\n",
        "            total_acc += (logits.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e249cd2c",
      "metadata": {
        "id": "e249cd2c"
      },
      "source": [
        "### <font size='4' color='red'>Task 1.6: Define the model and loss function to train the model (3 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0112151f",
      "metadata": {
        "id": "0112151f"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "#assert torch.cuda.is_available()\n",
        "# device = 'cuda'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# Hyper parameters\n",
        "epochs = 3 # epoch\n",
        "lr = 0.0005 # learning rate\n",
        "batch_size = 64 # batch size for training\n",
        "word_embed_dim = 64\n",
        "rnn_hidden_dim = 96\n",
        "\n",
        "train_iter = AG_NEWS(split='train')\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "print('num_class',num_class)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "model, loss_func = None, None\n",
        "###########################################################################\n",
        "# TODO: Deinfe the classifier and loss function.\n",
        "###########################################################################\n",
        "\n",
        "\n",
        "model = RNNClassifier(vocab_size, word_embed_dim, rnn_hidden_dim, num_class, 0)\n",
        "\n",
        "loss_func = torch.nn.functional.cross_entropy\n",
        "\n",
        "#raise NotImplementedError\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################\n",
        "\n",
        "# copy the model to the specified device (GPU)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, 1e-8)\n",
        "total_accu = None\n",
        "train_iter, test_iter = AG_NEWS()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = random_split(\n",
        "    train_dataset,\n",
        "    [num_train, len(train_dataset) - num_train]\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    split_train_, batch_size=batch_size,\n",
        "    shuffle=True, collate_fn=collate_batch\n",
        ")\n",
        "\n",
        "valid_dataloader = DataLoader(\n",
        "    split_valid_, batch_size=batch_size,\n",
        "    shuffle=False, collate_fn=collate_batch\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, batch_size=batch_size,\n",
        "    shuffle=False, collate_fn=collate_batch\n",
        ")\n",
        "\n",
        "# You should be able get a validation accuracy around 87%\n",
        "for epoch in range(1, epochs + 1):\n",
        "    print(epoch)\n",
        "    epoch_start_time = time.time()\n",
        "    train(model, train_dataloader, loss_func, device, 1)\n",
        "    accu_val = evaluate(model, valid_dataloader, loss_func, device)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "        scheduler.step()\n",
        "    else:\n",
        "        total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3582578",
      "metadata": {
        "id": "e3582578"
      },
      "source": [
        "## Part 2: Text Classification with Transformer Encoder (41 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4db1190d",
      "metadata": {
        "id": "4db1190d"
      },
      "source": [
        "### <font size='4' color='red'>Task 2.1: Implement the multi-head attention module (no for loops allowed, 10 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d932295",
      "metadata": {
        "id": "7d932295"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A module that computes multi-head attention given query, key, and value tensors.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, num_heads: int):\n",
        "        \"\"\"\n",
        "        Constructor.\n",
        "\n",
        "        Inputs:\n",
        "        - input_dim: Dimension of the input query, key, and value. Here we assume they all have\n",
        "          the same dimensions. But they could have different dimensions in other problems.\n",
        "        - num_heads: Number of attention heads\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        assert input_dim % num_heads == 0\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dim_per_head = input_dim // num_heads\n",
        "\n",
        "\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO: Define the linear transformation layers for key, value, and query.#\n",
        "        # Also define the output layer.\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "        self.linear_query = nn.Linear(self.input_dim, self.input_dim)\n",
        "        self.linear_key = nn.Linear(self.input_dim, self.input_dim)\n",
        "        self.linear_value = nn.Linear(self.input_dim, self.input_dim)\n",
        "        self.linear_out = nn.Linear(self.input_dim, self.input_dim)\n",
        "\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor=None):\n",
        "        \"\"\"\n",
        "        Compute the attended feature representations.\n",
        "        Inputs:\n",
        "        - query: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
        "          and C is the channel dimension\n",
        "        - key: Tensor of the shape BxLxC\n",
        "        - value: Tensor of the shape BxLxC\n",
        "        - mask: Tensor indicating where the attention should *not* be performed\n",
        "        \"\"\"\n",
        "        b = query.shape[0]\n",
        "\n",
        "        dot_prod_scores = None\n",
        "        ###########################################################################\n",
        "        # TODO: Compute the scores based on dot product between transformed query,#\n",
        "        # key, and value. You may find torch.matmul helpful, whose documentation  #\n",
        "        # can be found at                                                         #\n",
        "        # https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul#\n",
        "        # Remember to devide the doct product similarity scores by square root of #\n",
        "        # the channel dimension per head.\n",
        "        #                                                                         #\n",
        "        # Since no for loops are allowed here, think of how to use tensor reshape #\n",
        "        # to process multiple attention heads at the same time.                   #\n",
        "        ###########################################################################\n",
        "        q = self.linear_query(query)\n",
        "        k = self.linear_key(key)\n",
        "        v = self.linear_value(value)\n",
        "\n",
        "        num_class, seq_len, feature_dim = q.size()\n",
        "\n",
        "\n",
        "        q=q.reshape(num_class, seq_len, self.num_heads, self.dim_per_head).permute(0, 2, 1, 3)\n",
        "        q=q.reshape(num_class * self.num_heads, seq_len, self.dim_per_head)\n",
        "\n",
        "        k=q.reshape(num_class, seq_len, self.num_heads, self.dim_per_head).permute(0, 2, 1, 3)\n",
        "        k=k.reshape(num_class * self.num_heads, seq_len, self.dim_per_head)\n",
        "\n",
        "        v=q.reshape(num_class, seq_len, self.num_heads, self.dim_per_head).permute(0, 2, 1, 3)\n",
        "        v=v.reshape(num_class * self.num_heads, seq_len, self.dim_per_head)\n",
        "\n",
        "\n",
        "        dk = q.size()[-1]\n",
        "\n",
        "        dot_prod_scores = q.matmul(k.transpose(-2, -1)) / math.sqrt(dk)\n",
        "\n",
        "\n",
        "\n",
        "        # raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "        if mask is not None:\n",
        "            # We simply set the similarity scores to be near zero for the positions\n",
        "            # where the attention should not be done. Think of why we do this.\n",
        "            dot_prod_scores = dot_prod_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "\n",
        "        out = None\n",
        "        ###########################################################################\n",
        "        # TODO: Compute the attention scores, which are then used to modulate the #\n",
        "        # value tensor. Finally concate the attended tensors from multiple heads  #\n",
        "        # and feed it into the output layer. You may still find torch.matmul      #\n",
        "        # helpful.                                                                #\n",
        "        #                                                                         #\n",
        "        # Again, think of how to use reshaping tensor to do the concatenation.    #\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "        attention = F.softmax(dot_prod_scores, dim=-1)\n",
        "\n",
        "        y = attention.matmul(v)\n",
        "\n",
        "\n",
        "\n",
        "        num_class, seq_len, feature_dim = y.size()\n",
        "\n",
        "        num_class //= self.num_heads\n",
        "        out_dim = feature_dim * self.num_heads\n",
        "\n",
        "        y = y.reshape(num_class, self.num_heads, seq_len, feature_dim).permute(0, 2, 1, 3)\n",
        "        y = y.reshape(num_class, seq_len, out_dim)\n",
        "\n",
        "\n",
        "        out = self.linear_out(y)\n",
        "\n",
        "\n",
        "        #if self.activation is not None:\n",
        "         #   y = self.activation(y)\n",
        "        #return y\n",
        "\n",
        "\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DNM7I0oQElLG"
      },
      "id": "DNM7I0oQElLG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5bd2c45",
      "metadata": {
        "id": "c5bd2c45"
      },
      "outputs": [],
      "source": [
        "x = torch.randn((2, 10, 8))\n",
        "x = torch.randn(( 2,10, 8))\n",
        "\n",
        "mask = torch.randn((2, 10)) > 0.5\n",
        "mask = mask.unsqueeze(1)\n",
        "num_heads = 4\n",
        "model = MultiHeadAttention(8, num_heads)\n",
        "y = model(x, x, x)\n",
        "assert len(y.shape) == len(x.shape)\n",
        "for dim_x, dim_y in zip(x.shape, y.shape):\n",
        "    assert dim_x == dim_y\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02379104",
      "metadata": {
        "id": "02379104"
      },
      "source": [
        "### <font size='4' color='red'>Task 2.2: Implement a Feedforward Network (3 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b7083ff",
      "metadata": {
        "id": "5b7083ff"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple feedforward network. Essentially, it is a two-layer fully-connected\n",
        "    neural network.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, ff_dim, dropout):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - input_dim: Input dimension\n",
        "        - ff_dim: Hidden dimension\n",
        "        \"\"\"\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO: Define the two linear layers and a non-linear one.\n",
        "        ###########################################################################\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = ff_dim\n",
        "        self.linear_1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
        "        self.linear_2 = nn.Linear(self.hidden_dim, self.input_dim)\n",
        "        self.nonlinear= F.relu\n",
        "\n",
        "\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
        "         and C is the channel dimension\n",
        "\n",
        "        Return:\n",
        "        - y: Tensor of the shape BxLxC\n",
        "        \"\"\"\n",
        "\n",
        "        y = None\n",
        "        ###########################################################################\n",
        "        # TODO: Process the input.                                                #\n",
        "        ###########################################################################\n",
        "\n",
        "        h = self.linear_1(x)\n",
        "        out =  self.linear_2(h)\n",
        "        y = self.nonlinear(out)\n",
        "\n",
        "\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1671c24f",
      "metadata": {
        "id": "1671c24f"
      },
      "outputs": [],
      "source": [
        "x = torch.randn((2, 10, 8))\n",
        "ff_dim = 4\n",
        "model = FeedForwardNetwork(8, ff_dim, 0.1)\n",
        "y = model(x)\n",
        "assert len(x.shape) == len(y.shape)\n",
        "for dim_x, dim_y in zip(x.shape, y.shape):\n",
        "    assert dim_x == dim_y\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daa64217",
      "metadata": {
        "id": "daa64217"
      },
      "source": [
        "### <font size='4' color='red'>Task 2.3: Implement a Single Transformer Encoder Cell (8 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5aca2a1",
      "metadata": {
        "id": "c5aca2a1"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderCell(nn.Module):\n",
        "    \"\"\"\n",
        "    A single cell (unit) for the Transformer encoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, dropout: float):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - input_dim: Input dimension for each token in a sequence\n",
        "        - num_heads: Number of attention heads in a multi-head attention module\n",
        "        - ff_dim: The hidden dimension for a feedforward network\n",
        "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
        "          modules.\n",
        "        \"\"\"\n",
        "        super(TransformerEncoderCell, self).__init__()\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO: A single Transformer encoder cell consists of\n",
        "        # 1. A multi-head attention module\n",
        "        # 2. Followed by dropout\n",
        "        # 3. Followed by layer norm (check nn.LayerNorm)\n",
        "        # https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm\n",
        "        #                                                                         #\n",
        "        # At the same time, it also has\n",
        "        # 1. A feedforward network\n",
        "        # 2. Followed by dropout\n",
        "        # 3. Followed by layer norm\n",
        "        ###########################################################################\n",
        "        self.input_dim = input_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_dim = ff_dim\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.norm_1 = nn.LayerNorm(input_dim)\n",
        "        self.norm_2 = nn.LayerNorm(input_dim)\n",
        "\n",
        "        self.MHA = MultiHeadAttention(self.input_dim, self.num_heads)\n",
        "        self.ff = FeedForwardNetwork(input_dim, ff_dim, dropout=dropout)\n",
        "\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: torch.Tensor=None):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
        "          and C is the channel dimension\n",
        "        - mask: Tensor for multi-head attention\n",
        "        \"\"\"\n",
        "\n",
        "        y = None\n",
        "        ###########################################################################\n",
        "        # TODO: Get the output of the multi-head attention part (with dropout     #\n",
        "        # and layer norm), which is used as input to the feedforward network (    #\n",
        "        # again, followed by dropout and layer norm).                             #\n",
        "        #                                                                         #\n",
        "        # Don't forget the residual connections for both parts.                   #\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "        attention = self.MHA(x,x,x)\n",
        "\n",
        "        y0 = self.dropout_1 (attention)\n",
        "\n",
        "        y1 = self.norm_1 (x+y0)\n",
        "\n",
        "        y2 = self.ff (y1)\n",
        "\n",
        "        y3 = self.dropout_2 (y2)\n",
        "\n",
        "        y = self.norm_2 (y3 + y1)\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77e4fcff",
      "metadata": {
        "id": "77e4fcff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc6647ee-9bea-47d3-c744-8a94835b260b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10, 8])\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn((2, 10, 8))\n",
        "mask = torch.randn((2, 10)) > 0.5\n",
        "mask = mask.unsqueeze(1).unsqueeze(-1)\n",
        "num_heads = 4\n",
        "model = TransformerEncoderCell(8, num_heads, 32, 0.1)\n",
        "y = model(x, mask)\n",
        "assert len(x.shape) == len(y.shape)\n",
        "for dim_x, dim_y in zip(x.shape, y.shape):\n",
        "    assert dim_x == dim_y\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0944cd8",
      "metadata": {
        "id": "b0944cd8"
      },
      "source": [
        "### <font size='4' color='red'>Task 2.4: Implement Transformer Encoder (5 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f72b398a",
      "metadata": {
        "id": "f72b398a"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A full encoder consisting of a set of TransformerEncoderCell.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, num_cells: int, dropout: float=0.1):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - input_dim: Input dimension for each token in a sequence\n",
        "        - num_heads: Number of attention heads in a multi-head attention module\n",
        "        - ff_dim: The hidden dimension for a feedforward network\n",
        "        - num_cells: Number of TransformerEncoderCells\n",
        "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
        "          modules.\n",
        "        \"\"\"\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.norm = None\n",
        "        ###########################################################################\n",
        "        # TODO: Construct a nn.ModuleList to store a stack of                     #\n",
        "        # TranformerEncoderCells. Check the documentation here of how to use it   #\n",
        "        # https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList\n",
        "\n",
        "        # At the same time, define a layer normalization layer to process the     #\n",
        "        # output of the entire encoder.                                           #\n",
        "        ###########################################################################\n",
        "        self.input_dim = input_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_dim = ff_dim\n",
        "        self.num_cells = num_cells\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.encoderCell = TransformerEncoderCell(self.input_dim, self.num_heads, self.hidden_dim, self.dropout)\n",
        "\n",
        "        self.encoders = nn.ModuleList([self.encoderCell for i in range(self.num_cells)])\n",
        "\n",
        "        self.norm = nn.LayerNorm(self.input_dim)\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: torch.Tensor=None):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
        "          and C is the channel dimension\n",
        "        - mask: Tensor for multi-head attention\n",
        "\n",
        "        Return:\n",
        "        - y: Tensor of the shape of BxLxC, which is the normalized output of the encoder\n",
        "        \"\"\"\n",
        "\n",
        "        y = None\n",
        "        ###########################################################################\n",
        "        # TODO: Feed x into the stack of TransformerEncoderCells and then         #\n",
        "        # normalize the output with layer norm.                                   #\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "        for i in range(self.num_cells):\n",
        "            x = self.encoders[i](x, mask)\n",
        "\n",
        "\n",
        "        y = self.norm(x)\n",
        "\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64ddad9c",
      "metadata": {
        "id": "64ddad9c"
      },
      "outputs": [],
      "source": [
        "x = torch.randn((2, 10, 8))\n",
        "mask = torch.randn((2, 10)) > 0.5\n",
        "mask = mask.unsqueeze(1).unsqueeze(-1)\n",
        "num_heads = 4\n",
        "model = TransformerEncoder(8, num_heads, 32, 2, 0.1)\n",
        "y = model(x)\n",
        "assert len(x.shape) == len(y.shape)\n",
        "for dim_x, dim_y in zip(x.shape, y.shape):\n",
        "    assert dim_x == dim_y\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4c4330c",
      "metadata": {
        "id": "b4c4330c"
      },
      "source": [
        "### <font size='4' color='red'>Task 2.5: Implement Positional Encoding (7 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52b5ff70",
      "metadata": {
        "id": "52b5ff70"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    A module that adds positional encoding to each of the token's features.\n",
        "    So that the Transformer is position aware.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, max_len: int=10000):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - input_dim: Input dimension about the features for each token\n",
        "        - max_len: The maximum sequence length\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.max_len = max_len\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Compute the positional encoding and add it to x.\n",
        "\n",
        "        Input:\n",
        "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
        "          and C is the channel dimension\n",
        "\n",
        "        Return:\n",
        "        - x: Tensor of the shape BxLxC, with the positional encoding added to the input\n",
        "        \"\"\"\n",
        "        seq_len = x.shape[1]\n",
        "        input_dim = x.shape[2]\n",
        "\n",
        "        pe = None\n",
        "        ###########################################################################\n",
        "        # TODO: Compute the positional encoding                                   #\n",
        "        # Check Section 3.5 for the definition (https://arxiv.org/pdf/1706.03762.pdf)\n",
        "        #                                                                         #\n",
        "        # It's a bit messy, but the definition is provided for your here for your #\n",
        "        # convenience (in LaTex).                                                 #\n",
        "        # PE_{(pos,2i)} = sin(pos / 10000^{2i/\\dmodel})                           #\n",
        "        # PE_{(pos,2i+1)} = cos(pos / 10000^{2i/\\dmodel})                         #\n",
        "        #                                                                         #\n",
        "        # You should replace 10000 with max_len here.\n",
        "        ###########################################################################\n",
        "\n",
        "        pe = torch.zeros(seq_len, input_dim)\n",
        "\n",
        "        for pos in range(seq_len):\n",
        "            for i in range(0, input_dim, 2):\n",
        "\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/input_dim)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/input_dim)))\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "\n",
        "        pe = pe[:,:seq_len]\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "        x = x + pe.to(x.device)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d1d76c1",
      "metadata": {
        "id": "6d1d76c1"
      },
      "outputs": [],
      "source": [
        "# Sanity check\n",
        "x = torch.randn(1, 100, 20)\n",
        "pe = PositionalEncoding(20)\n",
        "y = pe(x)\n",
        "assert len(x.shape) == len(y.shape)\n",
        "for dim_x, dim_y in zip(x.shape, y.shape):\n",
        "    assert dim_x == dim_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a34a7de",
      "metadata": {
        "id": "5a34a7de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "fc636ea6-5541-434d-c220-ed6870abdb9d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f2caf326c50>"
            ]
          },
          "metadata": {},
          "execution_count": 96
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "pe = PositionalEncoding(20)\n",
        "y = pe.forward((torch.zeros(1, 100, 20)))\n",
        "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
        "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "403b5eb6",
      "metadata": {
        "id": "403b5eb6"
      },
      "source": [
        "### <font size='4' color='red'>Task 2.6: Implement a Transformer-based Text Classifier (5 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea75af7f",
      "metadata": {
        "id": "ea75af7f"
      },
      "outputs": [],
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    A Transformer-based text classifier.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "            vocab_size: int, embed_dim: int, num_heads: int, trx_ff_dim: int,\n",
        "            num_trx_cells: int, num_class: int, dropout: float=0.1, pad_token: int=0\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - vocab_size: Vocabulary size, indicating how many tokens we have in total.\n",
        "        - embed_dim: The dimension of word embeddings\n",
        "        - num_heads: Number of attention heads in a multi-head attention module\n",
        "        - trx_ff_dim: The hidden dimension for a feedforward network\n",
        "        - num_trx_cells: Number of TransformerEncoderCells\n",
        "        - dropout: Dropout ratio\n",
        "        - pad_token: The index of the padding token.\n",
        "        \"\"\"\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # word embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_token)\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO: Define a module for positional encoding, Transformer encoder, and #\n",
        "        # a output layer                                                          #\n",
        "        ###########################################################################\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_heads = num_heads\n",
        "        self.trx_ff_dim = trx_ff_dim\n",
        "        self.num_trx_cells = num_trx_cells\n",
        "        self.num_class = num_class\n",
        "        self.dropout = dropout\n",
        "        self.pad_token = pad_token\n",
        "\n",
        "        self.TransformerEncoder = TransformerEncoder(self.embed_dim, self.num_heads, self.trx_ff_dim, self.num_trx_cells, self.dropout)\n",
        "\n",
        "        self.Positional = PositionalEncoding( self.embed_dim,self.vocab_size)\n",
        "\n",
        "        self.fc = nn.Linear(self.embed_dim,self.num_class)\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "    def forward(self, text, mask=None):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - text: Tensor with the shape of BxLxC.\n",
        "        - mask: Tensor for multi-head attention\n",
        "\n",
        "        Return:\n",
        "        - logits: Tensor with the shape of BxK, where K is the number of classes\n",
        "        \"\"\"\n",
        "\n",
        "        # word embeddings, note we multiple the embeddings by a factor\n",
        "\n",
        "\n",
        "\n",
        "        embedded = self.embedding(text) * math.sqrt(self.embed_dim)\n",
        "\n",
        "        seq_len = embedded.shape[1]\n",
        "\n",
        "\n",
        "        logits = None\n",
        "        ###########################################################################\n",
        "        # TODO: Apply positional embedding to the input, which is then fed into   #\n",
        "        # the encoder. Average pooling is applied then to all the features of all #\n",
        "        # tokens. Finally, the logits are computed based on the pooled features.  #\n",
        "        ###########################################################################\n",
        "\n",
        "        y0= self.Positional(embedded)\n",
        "\n",
        "\n",
        "        y1 = self.TransformerEncoder(y0)\n",
        "\n",
        "\n",
        "        yy = torch.mean(y1, 1)\n",
        "\n",
        "        logits= self.fc(yy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e926e90",
      "metadata": {
        "id": "5e926e90"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10\n",
        "embed_dim = 16\n",
        "num_heads = 4\n",
        "trx_ff_dim = 16\n",
        "num_trx_cells = 2\n",
        "num_class = 3\n",
        "\n",
        "x = torch.arange(vocab_size).view(1, -1)\n",
        "x = torch.cat((x, x), dim=0)\n",
        "mask = (x != 0).unsqueeze(-2).unsqueeze(1)\n",
        "model = TransformerClassifier(vocab_size, embed_dim, num_heads, trx_ff_dim, num_trx_cells, num_class)\n",
        "print('x: {}, mask: {}'.format(x.shape, mask.shape))\n",
        "y = model(x, mask)\n",
        "assert len(y.shape) == 2 and y.shape[0] == x.shape[0] and y.shape[1] == num_class\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c1f02fc",
      "metadata": {
        "id": "8c1f02fc"
      },
      "source": [
        "### <font size='4' color='red'>Task 2.7: Define the Model and Loss Function (3 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46bd5779",
      "metadata": {
        "id": "46bd5779"
      },
      "outputs": [],
      "source": [
        "#assert torch.cuda.is_available()\n",
        "# device = 'cuda'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 5 # epoch\n",
        "lr = 0.0005  # learning rate\n",
        "batch_size = 64 # batch size for training\n",
        "\n",
        "train_iter = AG_NEWS(split='train')\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "emsize = 64\n",
        "trx_ff_dim = 16\n",
        "num_heads = 4\n",
        "num_trx_cells = 2\n",
        "\n",
        "gradient_norm_clip = 1\n",
        "\n",
        "###########################################################################\n",
        "# Define a Transformer-based text classifier and a loss function.         #\n",
        "###########################################################################\n",
        "model = TransformerClassifier(vocab_size = vocab_size , embed_dim = emsize , num_heads = num_heads,\n",
        "            trx_ff_dim = trx_ff_dim, num_trx_cells = num_trx_cells, num_class = num_class)\n",
        "\n",
        "loss_func = torch.nn.functional.cross_entropy\n",
        "\n",
        "#raise NotImplementedError\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, 1e-8)\n",
        "total_accu = None\n",
        "\n",
        "# You should be able to get a validation accuracy around 89%\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model, train_dataloader, loss_func, device, gradient_norm_clip)\n",
        "    accu_val = evaluate(model, valid_dataloader, loss_func, device)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "        scheduler.step()\n",
        "    else:\n",
        "        total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fba2a914",
      "metadata": {
        "id": "fba2a914"
      },
      "source": [
        "## Part 3: Image Classification with Transformer (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8d670e6",
      "metadata": {
        "id": "b8d670e6"
      },
      "source": [
        "### <font size='4' color='red'>Task 3.1: Implement VisionTransformer for Image Classification (no flor loops are allowed, 7 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87161192",
      "metadata": {
        "id": "87161192"
      },
      "outputs": [],
      "source": [
        "class VisionTransformerClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    In the model, we partition an image into non-overlapping patches. Each patch is treated as a token.\n",
        "    We can get a sequence of such tokens by flattening the patches. Each token's embeddings is the\n",
        "    flattened RGB pixel values. If the patch size is 4, then the embeddings' dimension is 4*4*3.\n",
        "    You can check this paper https://arxiv.org/pdf/2010.11929.pdf for reference.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "            patch_size: int, num_heads: int, trx_ff_dim: int,\n",
        "            num_trx_cells: int, num_class: int, dropout: float=0.1\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - patch_size: Size of the non-overlapping patches\n",
        "        - num_heads: Number of attention heads\n",
        "        - trx_ff_dim: Hidden dimension of the feedforward network in a Transformer encoder\n",
        "        - num_trx_cells: Number of TransformerEncoderCells\n",
        "        - num_class: Number of image classes\n",
        "        - dropout: Dropout ratio\n",
        "        \"\"\"\n",
        "        super(VisionTransformerClassifier, self).__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO: Define a TransformerEncoder that takens non-overlapping patches   #\n",
        "        # of an image as input and another output layer for classification.       #\n",
        "        #                                                                         #\n",
        "        # Intuitively, we need 2D positional encodings for each patch according to#\n",
        "        # its x and y coordinates. But this reference paper https://arxiv.org/pdf/2010.11929.pdf\n",
        "        # shows there is no significance difference on accuracies. It is bit      #\n",
        "        # weird. But you can simply use the 1D positional encoding you have       #\n",
        "        # implemented earlier. You can experiment with 2D positional encodings    #\n",
        "        # if you like to earn extra credits.                                      #\n",
        "        ###########################################################################\n",
        "\n",
        "        embed_dim = self.patch_size * self.patch_size * 3\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.trx_ff_dim = trx_ff_dim\n",
        "        self.num_trx_cells = num_trx_cells\n",
        "        self.num_class = num_class\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.TransformerEncoder = TransformerEncoder(self.embed_dim, self.num_heads, self.trx_ff_dim, self.num_trx_cells, self.dropout)\n",
        "\n",
        "        self.Positional = PositionalEncoding( self.embed_dim,self.patch_size)\n",
        "\n",
        "        self.fc = nn.Linear(self.embed_dim,self.num_class)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "#     def init_weights(self):\n",
        "#         initrange = 0.5\n",
        "#         self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "#         self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "#         self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, image: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "        - image: Tensor of the shape BxCxHxW, where H and W are the height and width, respectively.\n",
        "\n",
        "        Return:\n",
        "        - logtis: Classification logits\n",
        "        \"\"\"\n",
        "\n",
        "        b, c, h, w = image.shape\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO: Partition an image into non-overlapping patches. Think of how to  #\n",
        "        # reshape the tensor to convert it to be the BxLxC format, which we have  #\n",
        "        # extensively used for NLP tasks. You may find tensor.permute helpful.    #\n",
        "        # Check documentation here https://pytorch.org/docs/stable/generated/torch.permute.html#torch.permute\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        patch_embed = torch.nn.modules.conv.Conv2d(in_channels=3,out_channels=self.embed_dim, kernel_size=self.patch_size, stride=self.patch_size)\n",
        "        patch_embed = patch_embed.to(image.device)\n",
        "\n",
        "        patch_embedd = patch_embed (image)\n",
        "\n",
        "        patch_embedd = patch_embedd.permute(0, 2, 3, 1)\n",
        "\n",
        "        b, h, w, c = patch_embedd.shape\n",
        "\n",
        "        l=h * w\n",
        "\n",
        "        patch_embedd = patch_embedd.reshape(b, l, c)\n",
        "\n",
        "        x = self.Positional(patch_embedd)\n",
        "\n",
        "        y = self.TransformerEncoder (x)\n",
        "\n",
        "        yy = torch.mean(y, 1)\n",
        "\n",
        "        logits= self.fc(yy)\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c0a5642",
      "metadata": {
        "id": "1c0a5642"
      },
      "outputs": [],
      "source": [
        "# Sanity check\n",
        "image = torch.randn((2, 3, 32, 32))\n",
        "patch_size = 4\n",
        "num_heads = 4\n",
        "num_trx_cells = 2\n",
        "trx_ff_dim = 16\n",
        "dropout = 0.1\n",
        "num_class = 5\n",
        "\n",
        "vit = VisionTransformerClassifier(patch_size, num_heads, trx_ff_dim, num_trx_cells, num_class, dropout)\n",
        "logits = vit(image)\n",
        "assert len(logits.shape) == 2 and logits.shape[0] == image.shape[0] and logits.shape[1] == num_class\n",
        "print(logits.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "966276ca",
      "metadata": {
        "id": "966276ca"
      },
      "source": [
        "### Prepare Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b80c3d63",
      "metadata": {
        "id": "b80c3d63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a30fbb29-7279-4948-faf4-eb803857bbc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '../datasets'\n",
            "/content\n",
            "--2022-04-20 22:30:36--  http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  13.3MB/s    in 11s     \n",
            "\n",
            "2022-04-20 22:30:47 (14.6 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n",
            "cifar-10-batches-py/\n",
            "cifar-10-batches-py/data_batch_4\n",
            "cifar-10-batches-py/readme.html\n",
            "cifar-10-batches-py/test_batch\n",
            "cifar-10-batches-py/data_batch_3\n",
            "cifar-10-batches-py/batches.meta\n",
            "cifar-10-batches-py/data_batch_2\n",
            "cifar-10-batches-py/data_batch_5\n",
            "cifar-10-batches-py/data_batch_1\n"
          ]
        }
      ],
      "source": [
        "# let's download the data\n",
        "%cd ../datasets\n",
        "\n",
        "# 1 -- Linux\n",
        "# 2 -- MacOS\n",
        "# 3 -- Command Prompt on Windows\n",
        "# 4 -- manually downloading the data\n",
        "choice = 1\n",
        "\n",
        "\n",
        "if choice == 1:\n",
        "    # should work well on Linux and in Powershell on Windows\n",
        "    !wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "elif choice == 2 or choice ==3:\n",
        "    # if wget is not available for you, try curl\n",
        "    # should work well on MacOS\n",
        "    !curl http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz --output cifar-10-python.tar.gz\n",
        "else:\n",
        "    print('Please manually download the data from http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz and put it under the datasets folder.')\n",
        "!tar -xzvf cifar-10-python.tar.gz\n",
        "\n",
        "if choice==3:\n",
        "    !del cifar-10-python.tar.gz\n",
        "else:\n",
        "    !rm cifar-10-python.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "724c00ec",
      "metadata": {
        "id": "724c00ec"
      },
      "outputs": [],
      "source": [
        "from six.moves import cPickle as pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from imageio import imread\n",
        "import platform\n",
        "\n",
        "def load_pickle(f):\n",
        "    version = platform.python_version_tuple()\n",
        "    if version[0] == '2':\n",
        "        return  pickle.load(f)\n",
        "    elif version[0] == '3':\n",
        "        return  pickle.load(f, encoding='latin1')\n",
        "    raise ValueError(\"invalid python version: {}\".format(version))\n",
        "\n",
        "def load_CIFAR_batch(filename):\n",
        "  \"\"\" load single batch of cifar \"\"\"\n",
        "  with open(filename, 'rb') as f:\n",
        "    datadict = load_pickle(f)\n",
        "    X = datadict['data']\n",
        "    Y = datadict['labels']\n",
        "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
        "    Y = np.array(Y)\n",
        "    return X, Y\n",
        "\n",
        "def load_CIFAR10(ROOT):\n",
        "  \"\"\" load all of cifar \"\"\"\n",
        "  xs = []\n",
        "  ys = []\n",
        "  for b in range(1,6):\n",
        "    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
        "    X, Y = load_CIFAR_batch(f)\n",
        "    xs.append(X)\n",
        "    ys.append(Y)\n",
        "  Xtr = np.concatenate(xs)\n",
        "  Ytr = np.concatenate(ys)\n",
        "  del X, Y\n",
        "  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
        "  return Xtr, Ytr, Xte, Yte\n",
        "\n",
        "\n",
        "def get_CIFAR10_data(cifar10_dir, num_training=49000, num_validation=1000, num_test=1000,\n",
        "                     subtract_mean=True):\n",
        "    \"\"\"\n",
        "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
        "    it for classifiers. These are the same steps as we used for the SVM, but\n",
        "    condensed to a single function.\n",
        "    \"\"\"\n",
        "    # Load the raw CIFAR-10 data\n",
        "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "\n",
        "    # Subsample the data\n",
        "    mask = list(range(num_training, num_training + num_validation))\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    mask = list(range(num_training))\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    mask = list(range(num_test))\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "\n",
        "    # Normalize the data: subtract the mean image\n",
        "    if subtract_mean:\n",
        "      mean_image = np.mean(X_train, axis=0)\n",
        "      X_train -= mean_image\n",
        "      X_val -= mean_image\n",
        "      X_test -= mean_image\n",
        "\n",
        "    # Transpose so that channels come first\n",
        "    X_train = X_train.transpose(0, 3, 1, 2).copy()\n",
        "    X_val = X_val.transpose(0, 3, 1, 2).copy()\n",
        "    X_test = X_test.transpose(0, 3, 1, 2).copy()\n",
        "\n",
        "    # Package data into a dictionary\n",
        "    return {\n",
        "      'X_train': X_train, 'y_train': y_train,\n",
        "      'X_val': X_val, 'y_val': y_val,\n",
        "      'X_test': X_test, 'y_test': y_test,\n",
        "    }\n",
        "\n",
        "# Split the data into train, val, and test sets.\n",
        "# Check the get_CIFAR10_data function for more details\n",
        "cifar10_dir = 'cifar-10-batches-py'\n",
        "data = get_CIFAR10_data(cifar10_dir)\n",
        "for k, v in list(data.items()):\n",
        "    print(('%s: ' % k, v.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f770e8d3",
      "metadata": {
        "id": "f770e8d3"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import TensorDataset\n",
        "\n",
        "def make_dataloader(x, y, batch_size, is_train):\n",
        "    dataset = TensorDataset(\n",
        "        torch.from_numpy(y).long(),\n",
        "        torch.from_numpy(x).float()\n",
        "    )\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=is_train,\n",
        "        num_workers=2,\n",
        "        drop_last=is_train\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "train_loader = make_dataloader(data['X_train'], data['y_train'], 8, True)\n",
        "for idx, (lab, im) in enumerate(train_loader):\n",
        "    if idx > 1:\n",
        "        break\n",
        "    print(im.shape, lab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "570d32af",
      "metadata": {
        "id": "570d32af"
      },
      "source": [
        "### <font size='4' color='red'>Task 3.2: Define the Model and Loss Function (3 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81f44a98",
      "metadata": {
        "id": "81f44a98"
      },
      "outputs": [],
      "source": [
        "patch_size = 4\n",
        "embed_dim = 128\n",
        "num_heads = 4\n",
        "trx_ff_dim = 128\n",
        "num_trx_cells = 2\n",
        "num_class = 10\n",
        "\n",
        "##### Because it was near the deadline for submiting I did not run training to get the percentage\n",
        "\n",
        "###########################################################################\n",
        "# TODO: Define the model and loss function\n",
        "###########################################################################\n",
        "\n",
        "model = VisionTransformerClassifier(patch_size = patch_size , num_heads = num_heads,\n",
        "            trx_ff_dim = trx_ff_dim, num_trx_cells = num_trx_cells, num_class = num_class)\n",
        "\n",
        "loss_func = torch.nn.functional.cross_entropy\n",
        "\n",
        "\n",
        "#raise NotImplementedError\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################\n",
        "\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "model = model.to(device)\n",
        "\n",
        "batch_size = 16\n",
        "train_loader = make_dataloader(data['X_train'], data['y_train'], batch_size, True)\n",
        "val_loader = make_dataloader(data['X_test'], data['y_test'], batch_size, False)\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 5 # epoch\n",
        "lr = 0.001\n",
        "gradient_norm_clips = 0.1\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, 1e-8)\n",
        "total_accu = None\n",
        "\n",
        "# You should be able to get an accuracy around 36%\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model, train_loader, loss_func, device, gradient_norm_clip)\n",
        "    accu_val = evaluate(model, val_loader, loss_func, device)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "        scheduler.step()\n",
        "    else:\n",
        "        total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Because it was near the deadline for submiting I did not run training to get the percentage. But when you run it will not have any error you could see the accuracy.**\n"
      ],
      "metadata": {
        "id": "mUInli_yK9v_"
      },
      "id": "mUInli_yK9v_"
    },
    {
      "cell_type": "markdown",
      "id": "f0e6ea7e",
      "metadata": {
        "id": "f0e6ea7e"
      },
      "source": [
        "## Part 4: Machine Translation with Transformer (25 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6ad5d07",
      "metadata": {
        "id": "e6ad5d07"
      },
      "source": [
        "### <font size='4' color='red'>Task 4.1: Implement Transformer Decoder Cell (10 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32619440",
      "metadata": {
        "id": "32619440"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoderCell(nn.Module):\n",
        "    \"\"\"\n",
        "    A single cell (unit) of the Transformer decoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, dropout: float=0.1):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - input_dim: Input dimension for each token in a sequence\n",
        "        - num_heads: Number of attention heads in a multi-head attention module\n",
        "        - ff_dim: The hidden dimension for a feedforward network\n",
        "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
        "          modules.\n",
        "        \"\"\"\n",
        "        super(TransformerDecoderCell, self).__init__()\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO: Similar to the TransformerEncoderCell, define two                 #\n",
        "        # MultiHeadAttention modules. One for processing the tokens on the        #\n",
        "        # decoder side. The other for getting the attention across the encoder.   #\n",
        "        # and the decoder. Also define a feedforward network. Don't forget the    #\n",
        "        # Dropout and Layer Norm layers.                                          #\n",
        "        ###########################################################################\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_dim = ff_dim\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.norm_1 = nn.LayerNorm(input_dim)\n",
        "        self.norm_2 = nn.LayerNorm(input_dim)\n",
        "        self.norm_3 = nn.LayerNorm(input_dim)\n",
        "\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "\n",
        "        self.MHA1 = MultiHeadAttention(self.input_dim, self.num_heads)\n",
        "        self.MHA2 = MultiHeadAttention(self.input_dim, self.num_heads)\n",
        "\n",
        "\n",
        "        self.ff = FeedForwardNetwork(self.input_dim, self.hidden_dim, dropout=dropout)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - x: Tensor of BxLdxC, word embeddings on the decoder side\n",
        "        - encoder_output: Tensor of BxLexC, word embeddings on the encoder side\n",
        "        - src_mask: Tensor, masks of the tokens on the encoder side\n",
        "        - tgt_mask: Tensor, masks of the tokens on the decoder side\n",
        "\n",
        "        Return:\n",
        "        - y: Tensor of BxLdxC. Attended features for all tokens on the decoder side.\n",
        "        \"\"\"\n",
        "\n",
        "        y = None\n",
        "        ###########################################################################\n",
        "        # TODO: Compute the self-attended features for the tokens on the decoder  #\n",
        "        # side. Then compute the corss-attended features for the tokens on the    #\n",
        "        # decoder side to the encoded features, which are finally feed into the   #\n",
        "        # feedforward network                                                     #\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "        MHA1_out = self.MHA1(x,x,x)\n",
        "\n",
        "        y0 = self.dropout_1 (MHA1_out)\n",
        "\n",
        "        y0 = self.norm_1 (x+y0)\n",
        "\n",
        "\n",
        "        MHA2_out = self.MHA2(y0, encoder_output, encoder_output)\n",
        "        y1 = self.dropout_2 (MHA2_out)\n",
        "        y1 = self.norm_2 (y0+y1)\n",
        "\n",
        "        y = self.ff (y1)\n",
        "        y = self.dropout_3 (y)\n",
        "        y = self.norm_3 (y + y1)\n",
        "\n",
        "\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1fe2ae8",
      "metadata": {
        "id": "f1fe2ae8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb6c3a38-46bd-45af-8d3b-2507f0d68927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 10, 16])\n"
          ]
        }
      ],
      "source": [
        "dec_feats = torch.randn((3, 10, 16))\n",
        "dec_mask = torch.randn((3, 1, 10, 10)) > 0.5\n",
        "\n",
        "enc_feats = torch.randn((3, 12, 16))\n",
        "enc_mask = torch.randn((3, 1, 1, 12)) > 0.5\n",
        "\n",
        "model = TransformerDecoderCell(16, 2, 32, 0.1)\n",
        "z = model(dec_feats, enc_feats, enc_mask, dec_mask)\n",
        "assert len(z.shape) == len(dec_feats.shape)\n",
        "for dim_z, dim_x in zip(z.shape, dec_feats.shape):\n",
        "    assert dim_z == dim_x\n",
        "print(z.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36259c71",
      "metadata": {
        "id": "36259c71"
      },
      "source": [
        "### <font size='4' color='red'>Task 4.2: Implement Transformer Decoder (5 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db0a3d21",
      "metadata": {
        "id": "db0a3d21"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A TransformerDecoder is a stack of multiple TransformerDecoderCells and a Layer Norm.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, num_cells: int, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - input_dim: Input dimension for each token in a sequence\n",
        "        - num_heads: Number of attention heads in a multi-head attention module\n",
        "        - ff_dim: The hidden dimension for a feedforward network\n",
        "        - num_cells: How many TransformerDecoderCells in stack\n",
        "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
        "          modules.\n",
        "        \"\"\"\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO: Construct a nn.ModuleList to store a stack of                     #\n",
        "        # TranformerDecoderCells. Check the documentation here of how to use it   #\n",
        "        # https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList\n",
        "\n",
        "        # At the same time, define a layer normalization layer to process the     #\n",
        "        # output of the entire encoder.                                           #\n",
        "        ###########################################################################\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_dim = ff_dim\n",
        "        self.num_cells = num_cells\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.decoderCell = TransformerDecoderCell(self.input_dim, self.num_heads, self.hidden_dim, self.dropout)\n",
        "\n",
        "        self.decoders = nn.ModuleList([self.decoderCell for i in range(self.num_cells)])\n",
        "\n",
        "        self.norm = nn.LayerNorm(self.input_dim)\n",
        "\n",
        "\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - x: Tensor of BxLdxC, word embeddings on the decoder side\n",
        "        - encoder_output: Tensor of BxLexC, word embeddings on the encoder side\n",
        "        - src_mask: Tensor, masks of the tokens on the encoder side\n",
        "        - tgt_mask: Tensor, masks of the tokens on the decoder side\n",
        "\n",
        "        Return:\n",
        "        - y: Tensor of BxLdxC. Attended features for all tokens on the decoder side.\n",
        "        \"\"\"\n",
        "\n",
        "        y = None\n",
        "        ###########################################################################\n",
        "        # TODO: Feed x into the stack of TransformerDecoderCells and then         #\n",
        "        # normalize the output with layer norm.                                   #\n",
        "        ###########################################################################\n",
        "\n",
        "        for i in range(self.num_cells):\n",
        "            x = self.decoders[i](x, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "\n",
        "        y = self.norm(x)\n",
        "\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0af7e67d",
      "metadata": {
        "id": "0af7e67d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2e096b8-1d8d-412d-ce76-749fe16d4e7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 10, 16])\n"
          ]
        }
      ],
      "source": [
        "dec_feats = torch.randn((3, 10, 16))\n",
        "dec_mask = torch.randn((3, 1, 10, 10)) > 0.5\n",
        "\n",
        "enc_feats = torch.randn((3, 12, 16))\n",
        "enc_mask = torch.randn((3, 1, 1, 12)) > 0.5\n",
        "\n",
        "model = TransformerDecoder(16, 2, 32, 2, 0.1)\n",
        "z = model(dec_feats, enc_feats, enc_mask, dec_mask)\n",
        "assert len(z.shape) == len(dec_feats.shape)\n",
        "for dim_z, dim_x in zip(z.shape, dec_feats.shape):\n",
        "    assert dim_z == dim_x\n",
        "print(z.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08f0615f",
      "metadata": {
        "id": "08f0615f"
      },
      "source": [
        "### <font size='4' color='red'>Task 4.3: Implement a Transformer-based Sequence-to-sequence model (7 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cad7dabf",
      "metadata": {
        "id": "cad7dabf"
      },
      "outputs": [],
      "source": [
        "class Seq2SeqTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer-based sequence-to-sequence model.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "            num_encoder_layers: int, num_decoder_layers: int, embed_dim: int,\n",
        "            num_heads: int, src_vocab_size: int, tgt_vocab_size: int,\n",
        "            trx_ff_dim: int = 512, dropout: float = 0.1, pad_token: int=0\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - num_encoder_layers: How many TransformerEncoderCell in stack\n",
        "        - num_decoder_layers: How many TransformerDecoderCell in stack\n",
        "        - embed_dim: Word embeddings dimension\n",
        "        - num_heads: Number of attention heads\n",
        "        - src_vocab_size: Number of tokens in the source language vocabulary\n",
        "        - tgt_vocab_size: Number of tokens in the target language vocabulary\n",
        "        - trx_ff_dim: Hidden dimension in the feedforward network\n",
        "        - dropout: Dropout ratio\n",
        "        \"\"\"\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Word embeddings for both the source and target languages\n",
        "        self.src_token_embed = nn.Embedding(src_vocab_size, embed_dim, padding_idx=pad_token)\n",
        "        self.tgt_token_embed = nn.Embedding(tgt_vocab_size, embed_dim, padding_idx=pad_token)\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO: Define the positional encoding, encoder, decoder, and the output  #\n",
        "        # layer. Think of how many classes are in the output layer.               #\n",
        "        ###########################################################################\n",
        "\n",
        "        self.num_encoder_layers =   num_encoder_layers\n",
        "        self.num_decoder_layers = num_decoder_layers\n",
        "        self.embed_dim = embed_dim\n",
        "        self.src_vocab_size = src_vocab_size\n",
        "        self.tgt_vocab_size = tgt_vocab_size\n",
        "        self.num_heads = num_heads\n",
        "        self.trx_ff_dim = trx_ff_dim\n",
        "        self.dropout = dropout\n",
        "        self.pad_token = pad_token\n",
        "\n",
        "        self.num_class = num_class\n",
        "\n",
        "\n",
        "\n",
        "        self.TransformerEncoder = TransformerEncoder (self.embed_dim, self.num_heads, self.trx_ff_dim, self.num_encoder_layers, self.dropout)\n",
        "        self.TransformerDecoder = TransformerDecoder (self.embed_dim, self.num_heads, self.trx_ff_dim, self.num_decoder_layers, self.dropout)\n",
        "\n",
        "        self.Positional_src = PositionalEncoding( self.embed_dim,self.src_vocab_size)\n",
        "        self.Positional_tgt = PositionalEncoding( self.embed_dim,self.tgt_vocab_size)\n",
        "\n",
        "\n",
        "        self.fc = nn.Linear(self.embed_dim,self.tgt_vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "    def forward(self, src: torch.Tensor, tgt: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - src: Tensor of BxLe, word indexes in the source language\n",
        "        - tgt: Tensor of BxLd, word indexes in the target language\n",
        "        - src_mask: Tensor, masks of the tokens on the encoder side\n",
        "        - tgt_mask: Tensor, masks of the tokens on the decoder side\n",
        "\n",
        "        Return:\n",
        "        - y: Tensor of BxLdxK. K is the number of classes in the output.\n",
        "        \"\"\"\n",
        "\n",
        "        # Get word embeddings. Not they are scaled.\n",
        "        src_embed = self.src_token_embed(src) * math.sqrt(self.embed_dim)\n",
        "        tgt_embed = self.tgt_token_embed(tgt) * math.sqrt(self.embed_dim)\n",
        "\n",
        "        logits = None\n",
        "        ###########################################################################\n",
        "        # TODO: Add positional encodings to the word embeddings. Feed them then   #\n",
        "        # to the encoder and decoder, respectively. Get the logits finally.       #\n",
        "        ###########################################################################\n",
        "\n",
        "        encoder_input = self.Positional_src (src_embed)\n",
        "        decoder_input = self.Positional_tgt (tgt_embed)\n",
        "\n",
        "\n",
        "        encoder_output = self.TransformerEncoder (encoder_input, src_mask)\n",
        "        y = self.TransformerDecoder ( decoder_input, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "\n",
        "\n",
        "        logits = self.fc(y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        ###########################################################################\n",
        "        #                             END OF YOUR CODE                            #\n",
        "        ###########################################################################\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ef6d873",
      "metadata": {
        "id": "5ef6d873",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85a73ec0-7be1-4e11-fb3e-c16a8773d530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 12, 12])\n"
          ]
        }
      ],
      "source": [
        "src_vocab_size = 10\n",
        "src = torch.arange(src_vocab_size).view(1, -1)\n",
        "src = torch.cat((src, src), dim=0)\n",
        "src_mask = torch.randn((2, 1, 1, src_vocab_size)) > 0.5\n",
        "\n",
        "tgt_vocab_size = 12\n",
        "tgt = torch.arange(tgt_vocab_size).view(1, -1)\n",
        "tgt = torch.cat((tgt, tgt), dim=0)\n",
        "tgt_mask = torch.randn((2, 1, tgt_vocab_size, tgt_vocab_size)) > 0.5\n",
        "\n",
        "model = Seq2SeqTransformer(2, 2, 16, 2, src_vocab_size, tgt_vocab_size, 32, 0.1, 0)\n",
        "z = model(src, tgt, src_mask, tgt_mask)\n",
        "print(z.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "922361b2",
      "metadata": {
        "id": "922361b2"
      },
      "source": [
        "### Create Attention Masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9a73cc5",
      "metadata": {
        "id": "c9a73cc5"
      },
      "outputs": [],
      "source": [
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0\n",
        "\n",
        "\n",
        "def create_mask(src, tgt, pad_token=0):\n",
        "    src_mask = (src != pad_token).unsqueeze(-2).unsqueeze(1)\n",
        "\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "    tgt_mask = (tgt != pad_token).unsqueeze(-2)\n",
        "    tgt_mask = tgt_mask & subsequent_mask(tgt.shape[1]).type_as(tgt_mask.data)\n",
        "\n",
        "    return src_mask, tgt_mask.unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebd199b2",
      "metadata": {
        "id": "ebd199b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "cbda54a9-c3a0-4241-e23a-1720a400e25a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1, 1, 10]) torch.Size([2, 1, 10, 10])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Let's visualize what the target mask looks like\n",
        "import seaborn\n",
        "seaborn.set_context(context=\"talk\")\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.imshow(subsequent_mask(20)[0].numpy())\n",
        "\n",
        "x = torch.arange(src_vocab_size).view(1, -1)\n",
        "x = torch.cat((x, x), dim=0)\n",
        "src_mask, tgt_mask = create_mask(x, x)\n",
        "print(src_mask.shape, tgt_mask.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efd3815f",
      "metadata": {
        "id": "efd3815f"
      },
      "source": [
        "### Prepare Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "276afbc7",
      "metadata": {
        "id": "276afbc7"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import Multi30k\n",
        "from typing import Iterable, List\n",
        "\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}\n",
        "\n",
        "\n",
        "# # Create source and target language tokenizer. Make sure to install the dependencies.\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "\n",
        "# helper function to yield list of tokens\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    # Training data Iterator\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    # Create torchtext's Vocab object\n",
        "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                    min_freq=1,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True)\n",
        "\n",
        "# Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
        "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    vocab_transform[ln].set_default_index(UNK_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7ac308f",
      "metadata": {
        "id": "e7ac308f"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# src and tgt language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(\n",
        "        token_transform[ln], #Tokenization\n",
        "        vocab_transform[ln], #Numericalization\n",
        "        tensor_transform # Add BOS/EOS and create tensor\n",
        "    )\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tesors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch.transpose(0, 1), tgt_batch.transpose(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "052976db",
      "metadata": {
        "id": "052976db"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "for idx, (src, tgt) in enumerate(train_dataloader):\n",
        "    if idx > 2:\n",
        "        break\n",
        "    print('src: {}, tgt: {}'.format(src.shape, tgt.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76e9d060",
      "metadata": {
        "id": "76e9d060"
      },
      "source": [
        "### <font size='4' color='red'>Task 4.4: Define the Model and Loss Function (3 points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1cfcf01",
      "metadata": {
        "id": "b1cfcf01"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMBED_SIZE = 512\n",
        "NUM_ATTN_HEADS = 8\n",
        "FF_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###########################################################################\n",
        "# TODO: Define the model and loss function.                               #\n",
        "# Note that this time we will generate tokens, where some of them in the  #\n",
        "# training time are from paddings. We don't want to penalize the model    #\n",
        "# if the output at such positions are wrong. You can use the              #\n",
        "# `ignore_index` in a loss function to suppress loss computation if the   #\n",
        "# ground-truth label is equal to the given value. Check here for          #\n",
        "# more details https://pytorch.org/docs/stable/nn.html#loss-functions     #\n",
        "###########################################################################\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS,\n",
        "                                 NUM_DECODER_LAYERS,\n",
        "                                 EMBED_SIZE,\n",
        "                                 NUM_ATTN_HEADS,\n",
        "                                 SRC_VOCAB_SIZE,\n",
        "                                 TGT_VOCAB_SIZE,\n",
        "                                 FF_DIM)\n",
        "\n",
        "\n",
        "loss_fn = torch.nn.functional.cross_entropy\n",
        "\n",
        "\n",
        "#raise NotImplementedError\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "transformer = transformer.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    transformer.parameters(),\n",
        "    lr=0.0001,\n",
        "    betas=(0.9, 0.98),\n",
        "    eps=1e-9\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14426704",
      "metadata": {
        "id": "14426704"
      },
      "source": [
        "### Model Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47d0266e",
      "metadata": {
        "id": "47d0266e"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
        "    print(train_dataloader.type)\n",
        "    for src, tgt in train_dataloader:\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "\n",
        "        tgt_input = tgt[:, :-1]\n",
        "\n",
        "        src_mask, tgt_mask = create_mask(src, tgt_input)\n",
        "        src_mask = src_mask.to(device)\n",
        "        tgt_mask = tgt_mask.to(device)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[:, 1:]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(train_dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
        "\n",
        "    for src, tgt in val_dataloader:\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "\n",
        "        tgt_input = tgt[:, :-1]\n",
        "\n",
        "        src_mask, tgt_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask)\n",
        "\n",
        "        tgt_out = tgt[:, 1:]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(val_dataloader)\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "\n",
        "# You should be able to get train loss around 1.5 and val loss around 2.2\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3660291b",
      "metadata": {
        "id": "3660291b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}